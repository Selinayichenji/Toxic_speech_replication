---
title: "Public's demand for moderation to online toxic speech is limited and influenced by social factors"
author: 
  - Yichen Ji
  - Xiaoxu Liu
thanks: "Code and data are available at: https://github.com/Selinayichenji/Toxic_speech_replication.git"
date: "`r format(Sys.time(), '%d %B %Y')`"
date-format: long
abstract: "In this report,we analysis the public's ideal moderation for 5 levels toxic online speech.It was found that most people tend to not give harsh moderations, unless the speech touches upon personal threat. And people with different social factors have difference preferences on moderations.The findings will help me build a ideal standard platform for the public."
format: pdf
toc: true
number-sections: true
bibliography: references.bib
---

```{r setup}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(readr)
library(knitr) 
library(kableExtra)
library(dplyr)
library(tidyr)
library(ggplot2)
library(MetBrewer)
library(grid)
```

```{r load}
#| echo: false
#| warning: false
#| message: false
#### Loading the Cleaned Dataset ###

lgbtq <- read_csv(here::here("data/analysis_data/lgbtq.csv"))

result_l_gender <-read_csv(here::here("data/analysis_data/lgbtq_gender.csv"))
result_l_race <-read_csv(here::here("data/analysis_data/lgbtq_race.csv"))
result_l_edu <-read_csv(here::here("data/analysis_data/lgbtq_education.csv"))
```

# Introduction
*This reproduction was performed after a replication on the Social Science Reproduction platform:****[link here](https://www.socialsciencereproduction.org/reproductions/92dbc146-6de1-48bf-b567-a91e33747595/index)***

  The research has shown that roughly 4 in 10 Americans have experienced online harassment, including name-calling, physical threats, and sexual abuse [@fouroften].  With the current widespread use of social media, the debation of regulating toxic content becomes even more pertinent. Cultivating civility within democratic discourse is strongly necessary, such as emphasizing respect and social order when communicating online. However, the emergence of uncivil, intolerant content on social platforms raises concerns about its potential harm to public discourse and democracy. Our focus is on the crucial dilemma: should measures be implemented to moderate toxic content and uphold civility, or should we suggest allowing such speech on social media to remain unconstrained? The pursuit of addressing these challenges becomes significant in shaping the future of online democratic engagement.
      
  Although hate, harassment, and extremism significantly impact the country and online community negatively as toxic comments have saturated lots of common social media in the U.S., the application of strong and effective regulation over social media faces challenges due to multiple factors. Various factors, including technology companies, government, and NGOs, oppose the potential heavy regulation, which operates within a distinct legal framework in the U.S. Besides, Users, the ultimate recipients of online toxicity, are important in reporting objectionable content through flagging mechanisms. Therefore, we attempt to find the ideal platform standards from users' views to against toxic speech.    
     
  We aim to apply the initial analysis from the original paper "Toxic Speech and Limited Demand for Content Moderation on Social Media", which is from the American Political Science Review. The paper attempts to figure out the consistency of how users reply to toxic speech when using social media to find an appropriate solution to improve the harmony of online platforms, and it includes two pieces of research: 1. targeting social groups and 2. targeting partisans.    
      
   In our reproduction, we use the original methods and the same dataset and shift the concentration from the level of toxic speech to the types of users while extending the research with more specific prevalent aspects of the groups of people's responses. Instead of talking about how people respond to toxic speech toward different labels of victims (LGBTQ, billionaire, and Christian) in study 1 of the original paper, we focused on how people with different genders, education levels, and races react to toxic speech toward LGBTQ. The estimand of our paper is differences of tendency of giving content moderation among people with different gender, race and education level when they face the toxic speech's target is LGBTQ.Beyond the changes, we hold all other perspectives to be the same as the original paper. 
      
       
  We obtain the result of the reproduction work and find that white people, people have postgraduate degree and man tend to give slighter moderation than other groups. Woman and black people tend to give serious moderation than others. The research result provides us with the information that:
 - In the traditional sense, socially advantaged groups tend to propose more lenient content moderation measures when the LGBTQ community is attacked, whereas disadvantaged groups are inclined to penalize speech that attacks the LGBTQ community.
- People usually want a more loose-controlled online environment and choose no heavy moderation toward heavy speech. 
      
  These help us understand the user preference in social media, and stimulate the appropriate regulations of the speeches on online platforms. Generally, the reproduction talks about the summary of what we do based on the original paper and what we obtain, the data sources, detailed pictures and analysis through coding, and the discussion that concludes our results and lessons while discussing (potential) drawbacks and anticipated regulations/behaviors in the future. 
  
  The paper introduces the basic information contains data source, methodology, variables and measurements in @sec-data. The visualization and analysis of the tendency differences in moderation by people across three social factors are presented in @sec-result. And in @sec-dis, we discuss what we have learned, our understanding of the world, the limitations, and the next steps of our research. @sec-app is for appendix and @sec-ref is listed all references in this paper.
  

# Data {#sec-data}

## Source

Our replication paper is based on the original paper in American Political Science Review  “Toxic Speech and Limited Demand for Content Moderation on Social Media". Our paper is consistent with the original goal that attempts to find how people respond to toxic speech with different targets to decide the moderation of social media for a respectful and harmonious environment. The dataset and replication package is open on the Harvard Dataverse website. For the data in LGBTQ target, the only esstential one is lgbtq.RData in the replication package.

The data came from the survey results collected by the authors of original paper. So there is no other similar datasets for same research.


## Methodology

Our paper applies using the statistical programming language R [@citeR]. Besides the programming tool, we also employ the following packages: readr [@read], ggplot2 [@gg2], dplyr [@dpr], tidyverse [@tidy], MetBrewer [@met], knitr [@citeknr], tidyr[@tdr], kableExtra[@kbe] and grid[@grid].


## Variables
We only introduced the variables used in our own analysis. For the full variables in survey from the original paper, please check the Appendix.

- Treatment: non-group-related control, control, uncivil, intolerant and threatening.
   - Non-group-related control means no target and no toxic language.
   - Control means anti-target but without the 3 kinds of toxic languages: uncivil, intolerant and threatening. 
   - Uncivil was defined as "including anything from an unnecessarily disrespectful tone and lack of respect to rudeness and inconsiderate language."[@paper]. 
   - The intolerance differs from incivility, "it aims to derogate, silence, or undermine particular groups due to their protected characteristics, attack their rights, and incite violence and harm."[@paper].
   - The threatening is a toxic behavior explicitly announces the intention of physical harm.[@paper].

- Handle: (1) Leave it, do nothing; (2) Place a warning label on the post; (3) Reduce how many people can see the post; (4) Permanently remove the post; (5) Suspend the person’s account.

- Gender: Male, Female and Others.
- Education: High school graduate; College; Postgraduate.
- Race: White, Black, Hispanic and Others.
- Percentage: It means the ratio of 5 handles given by people with the same social factor when they are facing the same treatment.

\newpage
## Data examples
```{r ex}
#| echo: false
#| warning: false
#| message: false

kable(head(lgbtq), "latex", booktabs = TRUE, caption = "Example of Analysis Data in LGBTQ Topic") %>%
  kable_styling(latex_options = c("striped", "scale_down", "hold_position"))
```

## Measurement

The data collected by using survey. In the study I (target social group), targets include LGBTQ, Billionaire and Christians.
The study measures participants' preferences for handling toxic speech on social media by asking them to react to posts on social media platforms. Specifically, the research measures participants' preferences through the following question:

- "In your view, how should social media companies (such as Facebook and Twitter) handle the post above?" Participants can choose from the following options:
   - "Leave it, do nothing"
   - "Place a warning label on the post"
   - "Reduce how many people can see the post"
   - "Permanently remove the post"
   - "Suspend the person’s account"

These options allow participants to express their preferences for the actions social media platforms should take regarding toxic speech. By observing participants' choices among these options, researchers can understand participants' varying preferences for content moderation on social media platforms under different experimental conditions.

The experiments took place in July 2022 (LGBTQ and Billionaires) and October 2022 (Christians). Each study recruited between 1,300 and 2,000 U.S. adults from the participant pool of the crowdsourcing platform Prolific. Participants were recruited according to specific procedures outlined in the APSR Dataverse (Pradel et al. 2024). Exclusion criteria, as pre-registered, included participants who failed the attention check, opted for exclusion from the study, or completed the survey in less than 50 seconds. Ethics approval was obtained from the Central University Research Ethics Committee of the University of Oxford, and the study design was pre-registered prior to data collection.[@paper]


\newpage
# Results {#sec-result}

We selected 3 social factors (gender, education level and race) to investigate people's moderation preference in LGBTQ Target toxic speech. 


```{r, fig.width=5.5, fig.height=4}
#| echo: false
#| eval: true
#| label: fig-lgbtq-rep
#| fig-cap: Preferred actions in response to distinct post types - LGBTQ
#| warning: false

data_wide <- lgbtq %>%
  count(treatment, handle) %>%
  pivot_wider(names_from = handle, values_from = n, values_fill = list(n = 0)) %>%
  mutate(total = rowSums(across(-treatment)))

data_wide <- data_wide %>%
  mutate(across(-c(treatment, total), ~ .x / total * 100))

data_long <- data_wide %>%
  pivot_longer(-c(treatment, total), names_to = "handle", values_to = "percent") %>%
  arrange(desc(treatment))

data_long <- data_long %>%
  mutate(treatment = factor(treatment, levels = c("non-group-related","control", "uncivil", "intolerant","threatening"), 
                            labels = c("No group mentioned", "Anti-target", "Uncivil post","Intolerant post","Threatening post")))


data_long$handle <- gsub("’", "'", data_long$handle)

ggplot(data_long, aes(x = fct_rev(treatment), y = percent, fill = handle)) +
  geom_bar(stat = "identity", width = 0.5) +
  theme_bw() +
  theme(
    plot.margin = margin(10.5, 20.5, 20.5, 5.5),
    legend.position = c(0.35, -0.17), 
    legend.key.size = unit(0.4, "lines"), 
    legend.text = element_text(size = 5), 
    legend.title = element_text(size = 7), 
    axis.title.y = element_text(face = "bold")
  ) +
  scale_fill_met_d(name = "Degas", direction = -1) +
  guides(
    fill = guide_legend(
      title = "How should social media \ncompanies handle the post?",
      title.position = "left",
      ncol = 3,
      byrow = TRUE
    )
  ) +
  labs(x = "", y = "Percent", fill = "") +
  labs(title = "Target: LGBTQ") +
  theme(
    plot.title = element_text(hjust = -0.295, size = 9, face = "bold")
  )+
  coord_flip() +
  scale_y_continuous(expand = expansion(add = c(0, 0)))
```

@fig-lgbtq-rep illustrates the percentages of 5 different handles in 5 levels toxic speech targets LGBTQ. As shown on the @fig-lgbtq-rep, the serious content moderations (Suspend account and Permanently remove the post) take more percentage as the toxic level increases. The interviewees are especially sensitive to threatening post to LGBTQ people. We can see that for no group mentioned post, most people choose to leave it, a few people put a warning. But in Threatening post, the percentages of permanently remove the post and suspend the account increase obviously, Interestingly, the indirect moderations'(Reduce posts' view and Put a warning label) percentages in threatening posts decrease compared to Intolerant post, but percentages direct moderations(Suspend account and Permanently remove the post) increase. This means people give up indirect method and more tend to take though and serious moderation.

In @fig-gender, @fig-race and @fig-educ, we visualize the difference of tendency of different groups' moderation. The plots arranges in order of the seriousness of moderation. Treatment is in the x-axis, ordered in posts' toxicity level, 1 to 5, from the lightest to the most toxic posts. Percentages are in y-axis. The percentage means what percent of people in specific social group takes such handle/moderation when they see the posts in x-axis. For example, the sum of all woman's percentages in Treatment 1 is 100%. If you are interested in specific numeric numbers, please check Table 3 to 5 in appendix.

```{r, fig.width=10, fig.height=6}
#| echo: false
#| eval: true
#| label: fig-gender
#| fig-cap: Percentage by Treatment and Gender in LGBTQ Topic
#| warning: false

result_l_gender$mapped_treatment <- as.integer(factor(result_l_gender$treatment, 
                                levels = c("non-group-related", "control", 
                                           "uncivil", "intolerant", "threatening")))

p <- ggplot(result_l_gender, aes(x = mapped_treatment, y = percentage, group = gender, color = gender)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = 1:5,  labels = c("1", "2", "3", "4", "5")) +
  facet_wrap(~handle) +
  theme_minimal() +
  labs(title = "", x = "Treatment", y = "Percentage") +
  scale_color_brewer(palette = "Set1")

p
grid.text("Treatment", x = 0.75, y = 0.42, just = "right")
grid.text("1: non-group-related", x = 0.85, y = 0.35, just = "right")
grid.text("2: control", x = 0.768, y = 0.3, just = "right")
grid.text("3: uncivil", x = 0.764, y = 0.25, just = "right")
grid.text("4: intolerant", x = 0.786, y = 0.2, just = "right")
grid.text("5: threatening", x = 0.8, y = 0.15, just = "right")

```


From @fig-gender, female tends to give more harsh moderation than male. 
Males are more inclined to take no actions than female in all toxic levels.As the toxicity of the posts increases, the tendency of females to do nothing significantly decreases, especially for threatening posts.
Both men and women are more inclined to permanently remove a post when it is threatening, but the tendency increases more sharply for women. As the toxicity of posts escalates from uncivil to threatening, both men and women are more likely to place a warning label, with women showing a slightly steeper increase in this tendency.
There is no significant gender difference for reducing posts' views, with both males and females less likely to choose this moderation for all toxicity levels of posts. Females are more likely than males to opt for suspending the account when dealing with threatening posts.
In contrast, males maintain a relatively stable tendency to moderate less than woman across all levels of post toxicity.

\newpage
```{r, fig.width=10, fig.height=6}
#| echo: false
#| eval: true
#| label: fig-race
#| fig-cap: Percentage by Treatment and Race in LGBTQ Topic
#| warning: false

result_l_race$mapped_treatment <- as.integer(factor(result_l_race$treatment, 
                                levels = c("non-group-related", "control", 
                                           "uncivil", "intolerant", "threatening")))

p <- ggplot(result_l_race, aes(x = mapped_treatment, y = percentage, group = race, color = race)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = 1:5, labels = c("1", "2", "3", "4", "5")) +
  facet_wrap(~handle) +
  theme_minimal() +
  labs(title = "", x = "Treatment", y = "Percentage") +
  scale_color_brewer(palette = "Set1")


p
grid.text("Treatment", x = 0.73, y = 0.42, just = "right")
grid.text("1: non-group-related", x = 0.83, y = 0.35, just = "right")
grid.text("2: control", x = 0.748, y = 0.3, just = "right")
grid.text("3: uncivil", x = 0.744, y = 0.25, just = "right")
grid.text("4: intolerant", x = 0.766, y = 0.2, just = "right")
grid.text("5: threatening", x = 0.78, y = 0.15, just = "right")

```

As shown on @fig-race, across all levels of post toxicity, Black individuals have the lowest tendency to take no action, especially noticeable in non-group-related (1) and control (2) posts, where their proportion of inaction rapidly decreases. Whites start with the highest proportion of inaction, which also decreases as the toxicity of the posts increases. Hispanics and Others have a moderate tendency not to act, which decreases with rising toxicity. 
When facing threatening posts (5), all racial groups show an increased tendency to remove the post, with Blacks and Hispanics showing the most significant upward trend.
As the toxicity of posts increases, all racial groups are more inclined to place warning labels on posts, with Whites and Others showing a more noticeable trend upwards.
There is little difference among racial groups in terms of reducing a post's visibility, with a general tendency not to choose this action, especially when dealing with less toxic posts.
When faced with threatening posts, the propensity to suspend accounts increases among all racial groups, with Blacks and Hispanics showing a particularly significant upward trend.
\newpage


```{r, fig.width=10, fig.height=6}
#| echo: false
#| eval: true
#| label: fig-educ
#| fig-cap: Percentage by Treatment and Education in LGBTQ Topic
#| warning: false

result_l_edu$mapped_treatment <- as.integer(factor(result_l_edu$treatment, 
                                levels = c("non-group-related", "control", 
                                           "uncivil", "intolerant", "threatening")))

p <- ggplot(result_l_edu, aes(x = mapped_treatment, y = percentage, group = educ, color = educ)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = 1:5, labels = c("1", "2", "3", "4", "5")) +
  facet_wrap(~handle) +
  theme_minimal() +
  labs(title = "", x = "Treatment", y = "Percentage") +
  scale_color_brewer(palette = "Set1")


p
grid.text("Treatment", x = 0.68, y = 0.42, just = "right")
grid.text("1: non-group-related", x = 0.78, y = 0.35, just = "right")
grid.text("2: control", x = 0.698, y = 0.3, just = "right")
grid.text("3: uncivil", x = 0.694, y = 0.25, just = "right")
grid.text("4: intolerant", x = 0.716, y = 0.2, just = "right")
grid.text("5: threatening", x = 0.73, y = 0.15, just = "right")

```

As shown on @fig-educ, the inclination to take no action across all toxicity levels is relatively similar among the three education levels, with a steady decline as the posts increase in toxicity.

At toxicity levels 1 to 2, there is no significant difference among the three educational backgrounds regarding the percentage who prefer not to take any action. At toxicity levels 3 to 4, the percentage of postgraduates who choose not to take any action is noticeably higher than the other two educational backgrounds. However, when the toxicity level is at its most severe, it is the postgraduates who have the smallest proportion unwilling to take action.
Comparing the other four graphs, it can be seen that at toxicity levels 3 to 4, postgraduates have a lower percentage only in terms of putting a warning label on content. For the other three moderation actions, there is no significant difference compared to the other two educational backgrounds. However, when the toxicity level reaches the threshold of being threatening, the percentages for permanently removing the post and placing a warning label are the highest among the three educational backgrounds. Interestingly, for the most severe measure—account suspension—the percentage is actually the lowest.

In conclusion, the chart suggests that as posts become more toxic, individuals across all educational backgrounds tend to adopt stricter moderation actions. However, the variations among different educational levels are modest, with postgraduates slightly more inclined towards slighter measures (compare to suspend the account) such as putting warning label and remove the posts when dealing with the most toxic posts.


\newpage




# Discussion {#sec-dis}

## What we did

We replicated 3 figures contains 2 panels "Target LGBTQ" and "Target: Billionaires" in Figure3 "Preferences for Content Moderation by Treatment and by Experiment in Study I" and 1 element, Figure 6 "Preferences for Specific Types of Content Moderation by Treatment in Study II" from the paper “Toxic Speech and Limited Demand for Content Moderation on Social Media". Only the "Target LGBTQ" is shown in the paper as @fig-lgbtq-rep. The other figures' replication are in the scripts folder in Github, please check the link in the first page.

Different from the original paper, we focused on the 3 social factors groups' difference of preference on content moderation by treatment. By using the percentages' differences, we avoids the issue of 

## What we acquire {#sec-first-point}

In the paper, the result represents a trend that users prefer to do nothing to the toxic speech even if they may be affected negatively. The first study indicates that people are more likely to react to a toxic speech by reducing how many people can see the post or suspending the account if the target is LGBTQ and there is not much difference if the targets are billionaires or Christians. The second study implies that Democrats may tend to protect LGBTQ more while Republicans care about Christians more. In contrast with these, our reproduction acquires more detailed conclusions from three slightly different perspectives. From gender sight, women show a stronger tendency to protect LGBTQ than men do. From the education level perspective, those who are high school graduates tend to exhibit more protective behaviors when dealing with higher levels of toxic speech while those who are postgraduate act more sensitively to the uncivil levels. From the race perspective, Blacks are usually more sensitive to the LGBTQ target overall.


## What we learn: Point 1: Diverse Consequences of Different Toxic Speech Types {#sec-second-point}
The study challenges past research by revealing various types of toxic speech, including incivility, intolerance, and violent threats. Unlike previous work that often focused solely on labeling manifestations of incivility, this research connects toxic speech types to their consequences. It introduces a nuanced perspective, suggesting that users perceive these speech types as distinct constructs. While intolerance and incivility prompt similar content moderation responses, the study highlights the empirical insight that users view them differently, considering incivility a matter of tone and intolerance a matter of substance, such as discrimination.


## Point 2: Limited Support for Content Moderation {#sec-third-point}
A significant finding is the overall low support for content moderation of uncivil and intolerant content. The majority of respondents express the view that such content should remain online, with censorious forms of moderation, like banning users or removing content, being among the least favored options. Even when presented with extreme cases of toxic speech, such as attacks on the LGBTQ community, a large portion of respondents do not advocate for content moderation. This raises concerns about the broader implications for public discourse, as a substantial portion of users seems reluctant to endorse content moderation, even in the face of highly objectionable speech.

## Point 3: Partisan Consistency and New Research Avenues {#sec-fourth-point}
Contrary to expectations in an era of affective polarization, the study finds limited evidence that users view moderation of toxic speech through partisan lenses. Democrats, in general, show a greater tendency to demand moderation, but the identity of the victim (Republican or Democrat) does not significantly influence partisans' views. This finding opens up a new research puzzle, suggesting that Americans' strong belief in the value of freedom of speech might be a driving factor. The study calls for further exploration into whether this trend persists in other countries with different legal frameworks. The results emphasize the need to understand content moderation preferences beyond partisan lines and suggest that Americans, despite political polarization, exhibit consistency in their views on this matter.

## Weaknesses {#sec-weak}
The primary limitation of our research stems from the lack of modeling. The absence of models means that we do not account for potential interactions between different social factors and how they may collectively influence moderation behaviors. This gap is significant because different social groups may have varying degrees of influence on content moderation decisions, and without proper weighting in our analysis, the results might not fully reflect the complexities of these interactions.

Moreover, another limitation is related to the representativeness of our sample. The percentages of different social factor groups in our survey may not accurately mirror the actual composition of society. This discrepancy can lead to skewed results, as the moderation tendencies we observe may be over or under-represented due to sampling bias. For example, if a social factor group that is more likely to advocate for stricter moderation is under-represented in the survey, our findings may underestimate the desire for such moderation across the population.

The survey results involve the interpretation of social tendencies toward content moderation, which are inherently nuanced and subject to the cultural and social dynamics within each group. However, our current approach does not allow us to explore these dynamics in depth, which could provide valuable insights into the reasons behind the observed moderation preferences.

## Next steps
To address these limitations, our next steps involve the introduction of statistical models, such as regression analyses, which can help us understand the weight and impact of each social factor on moderation decisions. Regression models would allow us to control for various covariates and examine the independent effect of each variable. By doing so, we can also investigate interaction effects, providing a more nuanced understanding of how multiple social factors may interplay to influence content moderation behaviors.

Additionally, efforts should be made to ensure that our sample is more reflective of society's actual demographic composition. This could involve stratified sampling or weighting survey responses to match the demographic makeup of the population. Implementing these methods will likely result in more generalizable and accurate findings that can better inform content moderation policies and practices.


\newpage

\appendix

# Appendix {#sec-app}

```{r}
#| echo: false
#| warning: false
#| message: false

# Your data as a tibble or data.frame
survey_questions <- tibble(
  Variable = c( "Dependent Variable", "... Support of any form of moderation", "Other Variables",
                "... Political Identity", "... Social media visits", 
                "... Age", "... Gender", "... Education", "... Race/Ethnicity", 
                "... Attention Check", "... Emotions", "... Preference for removing the post"),
  Question = c("","In your view, how should social media companies like Facebook and Twitter handle the post above?","",
               "Generally speaking, do you consider yourself as being a Republican, a Democrat or an Independent?",
               "Overall, how often would you say you visit social media platforms (Twitter, Facebook, etc.)?",
               "What is your age?",
               "What is your gender?",
               "What is your highest level of educational qualification?",
               "What is your ethnicity?",
               "Please indicate your agreement with the following statement below. For our survey, it is essential that participants pay attention. To show us that you are reading this, please select both “Somewhat agree” and “Strongly agree” here.",
               "Which of the following emotions best describe your feelings about this social media post?",
               "Overall, how strongly do you feel this post should be kept or removed?"),
  Response = c("","Leave it, do nothing (1), Place a warning label on the post (2), Reduce how many people can see the post (3), Permanently remove the post (4), Suspend the person's account (5)", "",
               "Strong Democrat (1), Democrat (2), Leaning Democrat (3), Independent (4), Leaning Republican (5), Republican (6), Strong Republican (7)",
               "Every day (1) At least once a week but not every day (2), A few times a month (3), Less often (4)",
               "[Open text box]",
               "Female (1), Male (2), Other (3)",
               "Less than high school (1), High school graduate (2), Professional degree (3), College (4), Postgraduate (e.g., Masters) (5), PhD (6)",
               "White (1), Black or African American (2), American Indian or Alaska Native (3), Asian (4), Native Hawaiian or Pacific Islander (5), Hispanic (6), Other (7)",
               "Strongly disagree (1), Somewhat disagree (2), Neither agree nor disagree (3), Somewhat agree (4), Strongly agree (5) (Multiple selection is possible)",
               "Anger (Slider 0-100), Enthusiasm (Slider 0-100), Disgust (Slider 0-100), Fear (Slider 0-100), Happiness (Slider 0-100)",
               "Slider 0 (Keep the post) - 100 (Remove the post)")
)

add_blank_rows <- function(data) {
  # Create a new tibble to hold rows with blanks
  new_data <- tibble(Variable = character(), Question = character(), Response = character())
  
  # Add rows from original data and intersperse with blank rows
  for (i in 1:nrow(data)) {
    new_data <- add_row(new_data, data[i, ])
    # Do not add a blank row after specific rows
    if (!data$Variable[i] %in% c("Dependent Variable", "Other Variables")) {
      new_data <- add_row(new_data, Variable = "", Question = "", Response = "")
    }
  }
  return(new_data)
}

# add blank rows
expanded_questions <- add_blank_rows(survey_questions)


kable_table <- kable(expanded_questions, format = "latex", 
                     booktabs = TRUE, escape = FALSE, caption = "Overview of survey questions and variables") %>%
  kable_styling(latex_options = c("hold_position"),font_size = 8) %>%
  column_spec(1, width = "4cm") %>%
  column_spec(2, width = "7cm") %>%
  column_spec(3, width = "6.3cm")

kable_table

```
\newpage
```{r gender_data}
#| echo: false
#| warning: false
#| message: false

result_l_gender<- result_l_gender %>%
  select(-mapped_treatment) %>%
  mutate(percentage = round(percentage, 2))

kable(result_l_gender, "latex", booktabs = TRUE, caption = "Actual Data of Gender Analysis", longtable = TRUE) %>%
  kable_styling(latex_options = c("striped", "hold_position"), font_size = 10)
```

```{r race_data}
#| echo: false
#| warning: false
#| message: false

result_l_race <- result_l_race %>%
  select(-mapped_treatment) %>%
  mutate(percentage = round(percentage, 2))

kable(result_l_race, "latex", booktabs = TRUE, caption = "Actual Data of Race Analysis", longtable = TRUE) %>%
  kable_styling(latex_options = c("striped", "hold_position"), font_size = 10)

```

```{r edu_data}
#| echo: false
#| warning: false
#| message: false

result_l_edu <- result_l_edu %>%
  select(-mapped_treatment) %>%
  mutate(percentage = round(percentage, 2))

kable(result_l_edu, "latex", booktabs = TRUE, caption = "Actual Data of Education Level Analysis", longtable = TRUE) %>%
  kable_styling(latex_options = c("striped", "hold_position"), font_size = 10)
```


\newpage


# References {#sec-ref}


