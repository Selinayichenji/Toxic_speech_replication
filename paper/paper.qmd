---
title: "Public's Ideal Moderation to Online Toxic Speech"
author: 
  - Yichen Ji
  - Xiaoxu Liu
thanks: "Code and data are available at: https://github.com/Selinayichenji/Toxic_speech_replication.git."
date: "`r format(Sys.time(), '%d %B %Y')`"
date-format: long
abstract: "In this report,we analysis the public's ideal moderation for 5 levels toxic online speech.It was found that most people tend not to give harsh moderations, unless the speech touches upon personal threat. And people with different social factors have difference preferences on moderations.The findings will help me build a ideal standard platform for the public."
format: pdf
toc: true
number-sections: true
bibliography: references.bib
---

```{r setup}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(MetBrewer)
library(cowplot)
library(grid)
```


# Introduction
*This reproduction was performed after a replication on the Social Science Reproduction platform:****[link here](https://www.socialsciencereproduction.org/reproductions/f4c25f83-2c8b-448e-be76-760e02694112/index)***


The research has shown that roughly 4 in 10 Americans have experienced online harassment, including name-calling, physical threats, and sexual abuse (Pew Research Center 2021a).  With the current widespread use of social media, the debation of regulating toxic content becomes even more pertinent. Cultivating civility within democratic discourse is strongly necessary, such as emphasizing respect and social order when communicating online. However, the emergence of uncivil, intolerant content on social platforms raises concerns about its potential harm to public discourse and democracy. Our focus is on the crucial dilemma: should measures be implemented to moderate toxic content and uphold civility, or should we suggest allowing such speech on social media to remain unconstrained? The pursuit of addressing these challenges becomes significant in shaping the future of online democratic engagement.
Although hate, harassment, and extremism significantly impact the country and online community negatively as toxic comments have saturated lots of common social media in the U.S., the application of strong and effective regulation over social media faces challenges due to multiple factors. Various factors, including technology companies, government, and NGOs, oppose the potential heavy regulation, which operates within a distinct legal framework in the U.S. Besides, Users, the ultimate recipients of online toxicity, are important in reporting objectionable content through flagging mechanisms. Therefore, we attempt to find the ideal platform standards from users' views to against toxic speech.

We aim to apply the initial analysis from the original paper "Toxic Speech and Limited Demand for Content Moderation on Social Media", which is from the American Political Science Review. The paper attempts to figure out the consistency of how users reply to toxic speech when using social media to find an appropriate solution to improve the harmony of online platforms, and it includes two pieces of research: 1). targeting social groups and 2). targeting partisans. In our reproduction, we use the original methods and the same dataset and shift the concentration from the level of toxic speech to the types of users while extending the research with more specific prevalent aspects of the groups of people's responses. Instead of talking about how people respond to toxic speech toward different labels of victims (LGBTQ, billionaire, and Christian) in study 1 of the original paper, we expand the study range to how people with different genders, education levels, and races react to toxic speech toward LGBTQ, billionaires, and so on. Beyond the changes, we hold all other perspectives to be the same as the original paper. For example, we set standards of toxic speech with different levels: incivility (disrespectful tone, lack of respect, rudeness, and inconsiderate language), intolerance (derogating, silencing, or undermining particular groups with specified labels), and violent threats(the tendency of physical harm). 

  We obtain the result of the reproduction work and find that women (gender), high school graduates (education), and Blacks / Hispanics (races) imply a tendency to be more sensitive to high-level toxic speech on social media; on the other side, men, high school graduates, and Blacks tend to be more sensitive to the billionaires target overall. The research result provides us with the information that 1). Different groups of people hold different attitudes and behaviors toward topic speech with different targets and 2). People usually want a more loose-controlled online environment and choose no heavy moderation toward heavy speech. These help us understand the user preference in social media, and stimulate the appropriate regulations of the speeches on online platforms. Generally, the reproduction talks about the summary of what we do based on the original paper and what we obtain, the data sources, detailed pictures and analysis through coding, and the discussion that concludes our results and lessons while discussing (potential) drawbacks and anticipated regulations/behaviors in the future.  (can be improved)
  
ESTIMATE



# Data {#sec-data}

## Source

Our replication paper is based on the original paper in American Political Science Review  “Toxic Speech and Limited Demand for Content Moderation on Social Media". Our paper is consistent with the original goal that attempts to find how people respond to toxic speech with different targets to decide the moderation of social media for a respectful and harmonious environment. 


This paper applies the same sources as the original paper does, which consists of Papacharissi (2004, in particular pages 261–7), Herbst (2010), and Boatright et al. (2019); national laws (Busch 2022; European Parliament 2022); Brandeis notes in Whitney v. California 1927, “Partisan Conflict over Content Moderation Is More Than Disagreement about Facts.” (Appel, Pan, and Roberts Reference Appel, Pan and Roberts 2023; Kozyreva et al. Reference Kozyreva, Herzog, Lewandowsky, Hertwig, Lorenz-Spreen, Leiser and Reifler 2023), BBC. 2012. “Reddit Will Not Ban ‘Distasteful’ Content, Chief Executive Says”, Bejan, Teresa M. 2017. Mere Civility. Cambridge, and so on.

Similar dataset


## Methodology

Our paper applies using the statistical programming language R [@citeR]. Besides the programming tool, we also employ the following packages: readr [@read], broom [@broom], ggplot2 [@gg2], dplyr [@dpr], tidyverse [@tidy], MetBrewer [@met], and knitr [@citeknr].


## Variables
We only introduced the variables used in our own analysis. For the full variables from the original paper, please check the Appendix.

- Treatment: non-group-related control, control, uncivil, intolerant and threatening.
   - Non-group-related control means no target and no toxic language.
Control means anti-target but without the 3 kinds of toxic languages: uncivil, intolerant and threatening. 
In original paper, uncivil is defined as "including anything from an unnecessarily disrespectful tone and lack of respect to rudeness and inconsiderate language."[@paper]. 
And the intolerance differs from incivility, "it aims to derogate, silence, or undermine particular groups due to their protected characteristics, attack their rights, and incite violence and harm."[@paper].
The threatening is a toxic behavior explicitly announces the intention of physical harm.[@paper].

- Handle: (1) Leave it, do nothing; (2) Place a warning label on the post; (3) Reduce how many people can see the post; (4) Permanently remove the post; (5) Suspend the person’s account.

- Gender: Male, Female and Others.
- Education: High school graduate; College; Postgraduate.
- Race: White, Black, Hispanic and Others.
- Percentage: It means the ratio of 5 handles given by people with the same social factor when they are facing the same treatment.


## Measurement
The data collected by using survey.

## Structure of the paper'remainder parts


\newpage
# Results {#sec-result}

Our results are summarized in @tbl-modelresults.

```{r}
#| echo: false
#| warning: false
#| message: false
#### Loading the Cleaned Dataset ###

lgbtq <- read_csv(here::here("data/analysis_data/lgbtq.csv"))
billion <- read_csv(here::here("data/analysis_data/billion.csv"))
partisans <- read_csv(here::here("data/analysis_data/partisans.csv"))

result_l_gender <-read_csv(here::here("data/analysis_data/lgbtq_gender.csv"))
result_l_race <-read_csv(here::here("data/analysis_data/lgbtq_race.csv"))
result_l_edu <-read_csv(here::here("data/analysis_data/lgbtq_education.csv"))

#result_b_gender <-read_csv(here::here("data/analysis_data/billion_gender.csv"))
#result_b_race <- read_csv(here::here("data/analysis_data/billion_race.csv"))
#result_b_edu <-read_csv(here::here("data/analysis_data/billion_education.csv"))

```

```{r, fig.width=6, fig.height=5.1}
#| echo: false
#| eval: true
#| label: fig-lgbtq-rep
#| warning: false

data_wide <- lgbtq %>%
  count(treatment, handle) %>%
  pivot_wider(names_from = handle, values_from = n, values_fill = list(n = 0)) %>%
  mutate(total = rowSums(across(-treatment)))

data_wide <- data_wide %>%
  mutate(across(-c(treatment, total), ~ .x / total * 100))

data_long <- data_wide %>%
  pivot_longer(-c(treatment, total), names_to = "handle", values_to = "percent") %>%
  arrange(desc(treatment))

data_long <- data_long %>%
  mutate(treatment = factor(treatment, levels = c("non-group-related","control", "uncivil", "intolerant","threatening"), 
                            labels = c("No group mentioned", "Anti-target", "Uncivil post","Intolerant post","Threatening post")))


data_long$handle <- gsub("’", "'", data_long$handle)

ggplot(data_long, aes(x = fct_rev(treatment), y = percent, fill = handle)) +
  geom_bar(stat = "identity",width = 0.5) +
  theme_bw()+
  theme(
    legend.position = "bottom",
    legend.key.size = unit(0.3, "lines"),
    axis.title.y = element_text(face = "bold")
  ) +
  scale_fill_met_d(name = "Degas", direction = -1)+
   guides(
    fill = guide_legend(title = "How should social media \ncompanies handle the post?",
                        title.position = "left",
                        ncol = 3,# Position the title at the top
                        byrow = TRUE)  # Arrange items by row
  ) +
  labs(x = "", y = "Percent", fill = "") +
  coord_flip() +
  scale_y_continuous(expand = expansion(add = c(0, 0)))
```

@fig-lgbtq-rep


```{r gender_plot, fig.width=10, fig.height=6}
#| echo: false
#| eval: true
#| label: gender comparision in LGBTQ
#| warning: false

result_l_gender$mapped_treatment <- as.integer(factor(result_l_gender$treatment, 
                                levels = c("non-group-related", "control", 
                                           "uncivil", "intolerant", "threatening")))

p <- ggplot(result_l_gender, aes(x = mapped_treatment, y = percentage, group = gender, color = gender)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = 1:5,  labels = c("1", "2", "3", "4", "5")) +
  facet_wrap(~handle) +
  theme_minimal() +
  labs(title = "Percentage by Treatment and Gender in LGBTQ Topic", x = "Treatment", y = "Percentage") +
  scale_color_brewer(palette = "Set1")

p
grid.text("Treatment", x = 0.75, y = 0.42, just = "right")
grid.text("1: non-group-related", x = 0.85, y = 0.35, just = "right")
grid.text("2: control", x = 0.768, y = 0.3, just = "right")
grid.text("3: uncivil", x = 0.764, y = 0.25, just = "right")
grid.text("4: intolerant", x = 0.786, y = 0.2, just = "right")
grid.text("5: threatening", x = 0.8, y = 0.15, just = "right")

```


```{r race_plot, fig.width=10, fig.height=6}
#| echo: false
#| eval: true
#| label: race comparision in LGBTQ
#| warning: false

result_l_race$mapped_treatment <- as.integer(factor(result_l_race$treatment, 
                                levels = c("non-group-related", "control", 
                                           "uncivil", "intolerant", "threatening")))

p <- ggplot(result_l_race, aes(x = mapped_treatment, y = percentage, group = race, color = race)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = 1:5, labels = c("1", "2", "3", "4", "5")) +
  facet_wrap(~handle) +
  theme_minimal() +
  labs(title = "Percentage by Treatment and Race in LGBTQ Topic", x = "Treatment", y = "Percentage") +
  scale_color_brewer(palette = "Set1")


p
grid.text("Treatment", x = 0.73, y = 0.42, just = "right")
grid.text("1: non-group-related", x = 0.83, y = 0.35, just = "right")
grid.text("2: control", x = 0.748, y = 0.3, just = "right")
grid.text("3: uncivil", x = 0.744, y = 0.25, just = "right")
grid.text("4: intolerant", x = 0.766, y = 0.2, just = "right")
grid.text("5: threatening", x = 0.78, y = 0.15, just = "right")

```


```{r educ_plot, fig.width=10, fig.height=6}
#| echo: false
#| eval: true
#| label: education comparision in LGBTQ
#| warning: false

result_l_edu$mapped_treatment <- as.integer(factor(result_l_edu$treatment, 
                                levels = c("non-group-related", "control", 
                                           "uncivil", "intolerant", "threatening")))

p <- ggplot(result_l_edu, aes(x = mapped_treatment, y = percentage, group = educ, color = educ)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = 1:5, labels = c("1", "2", "3", "4", "5")) +
  facet_wrap(~handle) +
  theme_minimal() +
  labs(title = "Percentage by Treatment and Education in LGBTQ Topic", x = "Treatment", y = "Percentage") +
  scale_color_brewer(palette = "Set1")


p
grid.text("Treatment", x = 0.68, y = 0.42, just = "right")
grid.text("1: non-group-related", x = 0.78, y = 0.35, just = "right")
grid.text("2: control", x = 0.698, y = 0.3, just = "right")
grid.text("3: uncivil", x = 0.694, y = 0.25, just = "right")
grid.text("4: intolerant", x = 0.716, y = 0.2, just = "right")
grid.text("5: threatening", x = 0.73, y = 0.15, just = "right")

```




```{r fig.width=12, fig.height=7.5}
#| echo: false
#| eval: true
#| warning: false

# Function to prepare data
prepare_data <- function(partisans, target_partisan) {
  partisans %>%
    filter(partisan == target_partisan | treatment == "non-group-related control") %>% 
    mutate(treatment = factor(treatment, 
                              levels = c("non-group-related control", "control", 
                                         "uncivil", "intolerant", "threatening"), 
                              labels = c("No group mentioned", "Anti-target", 
                                         "Uncivil post", "Intolerant post", "Threatening post"))) %>%
    count(treatment, handle) %>%
    pivot_wider(names_from = handle, values_from = n, values_fill = list(n = 0)) %>%
    mutate(total = rowSums(across(-treatment))) %>%
    mutate(across(-c(treatment, total), ~ .x / total * 100)) %>%
    pivot_longer(-c(treatment, total), names_to = "handle", values_to = "percent") %>%
    arrange(desc(treatment)) %>%
    mutate(handle = iconv(handle, from = "UTF-8", to = "ASCII", sub = "'")) # Convert non-ASCII characters to ASCII
}


# Function to generate plot with legend and bold title
generate_plot <- function(data_long, show_legend = FALSE) {
  p <- ggplot(data_long, aes(x = fct_rev(treatment), y = percent, fill = handle)) +
    geom_bar(stat = "identity", width = 0.5) +
    theme_bw() +
    theme(
      legend.position = "none",  # We will place the legend manually
      axis.title.x = element_blank(),
      axis.text.y = element_text(face = "bold"),
      axis.text.x = element_blank(),
      plot.margin = margin(5.5, 10, 5.5, 5.5)
    ) +
    scale_fill_met_d(name = "Degas", direction = -1) +
    labs(x = "", y = "Percent", fill = "") +
    coord_flip() +
    scale_y_continuous(expand = expansion(add = c(0, 0)))

  if (show_legend) {
    p <- p + theme(legend.position = "bottom") + 
      guides(
        fill = guide_legend(
          title = "How should social media companies \nhandle the post?",
          title.position = "left", 
          ncol = 3, 
          byrow = TRUE 
        )
      )
  }

  return(p)
}

democrats_data_long <- prepare_data(partisans, "target: democrats")
republicans_data_long <- prepare_data(partisans, "target: republicans")

# Generate plots for Democrats and Republicans without individual titles
democrats_plot <- generate_plot(democrats_data_long)
republicans_plot <- generate_plot(republicans_data_long)

# Generate a plot with a legend to extract it
legend_plot <- generate_plot(democrats_data_long, TRUE) # Title is irrelevant here

# Extract the legend from the legend plot
legend <- get_legend(legend_plot)

# Combine plots and align the labels correctly
combined_plot <- plot_grid(
  democrats_plot + theme(plot.margin = margin(10, 10, 40, 5.5)), # Increase the top margin
  republicans_plot + theme(plot.margin = margin(10, 10, 40, 5.5)),
  labels = c("Target: Democrats", "Target: Republicans"),
  label_size = 12,
  label_fontface = "bold",
  hjust = -0.5, # Adjust horizontal position of labels
  vjust = 1.5,  # Adjust vertical position of labels
  ncol = 2
)

# Place the legend below the plots
final_plot <- plot_grid(
  combined_plot,
  legend,
  ncol = 1,
  rel_heights = c(1, 0.2) # Adjust these values as needed
)

# Print the final plot
print(final_plot)

```



# Discussion

## What we acquire {#sec-first-point}

In the paper, the result represents a trend that users prefer to do nothing to the toxic speech even if they may be affected negatively. The first study indicates that people are more likely to react to a toxic speech by reducing how many people can see the post or suspending the account if the target is LGBTQ and there is not much difference if the targets are billionaires or Christians. The second study implies that Democrats may tend to protect LGBTQ more while Republicans care about Christians more. In contrast with these, our reproduction acquires more detailed conclusions from three slightly different perspectives. From gender sight, men tend to agree with higher punishment than women do toward billionaires from gender sight, while women show a stronger tendency to protect LGBTQ than men do. From the education level perspective, those who are high school graduates tend to exhibit more protective behaviors when dealing with higher levels of toxic speech while those who are postgraduate act more sensitively to the uncivil levels. From the race perspective, Blacks and Hispanics are usually more sensitive to the LGBTQ and billionaires target overall.


## What we learn: Diverse Consequences of Different Toxic Speech Types
The study challenges past research by revealing various types of toxic speech, including incivility, intolerance, and violent threats. Unlike previous work that often focused solely on labeling manifestations of incivility, this research connects toxic speech types to their consequences. It introduces a nuanced perspective, suggesting that users perceive these speech types as distinct constructs. While intolerance and incivility prompt similar content moderation responses, the study highlights the empirical insight that users view them differently, considering incivility a matter of tone and intolerance a matter of substance, such as discrimination.


## Point 2: Limited Support for Content Moderation
A significant finding is the overall low support for content moderation of uncivil and intolerant content. The majority of respondents express the view that such content should remain online, with censorious forms of moderation, like banning users or removing content, being among the least favored options. Even when presented with extreme cases of toxic speech, such as attacks on the LGBTQ community, a large portion of respondents do not advocate for content moderation. This raises concerns about the broader implications for public discourse, as a substantial portion of users seems reluctant to endorse content moderation, even in the face of highly objectionable speech.

## Point 3: Partisan Consistency and New Research Avenues
Contrary to expectations in an era of affective polarization, the study finds limited evidence that users view moderation of toxic speech through partisan lenses. Democrats, in general, show a greater tendency to demand moderation, but the identity of the victim (Republican or Democrat) does not significantly influence partisans' views. This finding opens up a new research puzzle, suggesting that Americans' strong belief in the value of freedom of speech might be a driving factor. The study calls for further exploration into whether this trend persists in other countries with different legal frameworks. The results emphasize the need to understand content moderation preferences beyond partisan lines and suggest that Americans, despite political polarization, exhibit consistency in their views on this matter.

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {-}





\newpage


# References


